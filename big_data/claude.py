import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

import warnings
warnings.filterwarnings("ignore")

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
btc_file = '../knn/Bitcoin_01.10.2017-20.03.2025_historical_data_coinmarketcap.csv'
eth_file = '../knn/Ethereum_01.12.2018-28.02.2025_historical_data_coinmarketcap.csv'

df_btc = pd.read_csv(btc_file, sep=';')
df_eth = pd.read_csv(eth_file, sep=';')

# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –∑ –æ–∫—Ä–µ–º–∏–º–∏ —Å–∫–µ–π–ª–µ—Ä–∞–º–∏
feature_scaler = MinMaxScaler()
target_scaler = MinMaxScaler()

scaled_df = df_btc.copy()

# –í–∏–¥–∞–ª—è—î–º–æ –Ω–µ–øot—Ä—ñ–±–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
columns_to_drop = ['timeOpen', 'timeClose', 'timeHigh', 'timeLow']
for col in columns_to_drop:
    if col in scaled_df.columns:
        scaled_df.drop(columns=[col], inplace=True)

# –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ timestamp —è–∫ —ñ–Ω–¥–µ–∫—Å
if 'timestamp' in scaled_df.columns:
    scaled_df = scaled_df.set_index('timestamp')

# –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è features (–≤—Å–µ –æ–∫—Ä—ñ–º close)
feature_columns = ['open', 'high', 'low', 'volume', 'marketCap']
target_column = 'close'

# –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –≤—Å—ñ –∫–æ–ª–æ–Ω–∫–∏ —ñ—Å–Ω—É—é—Ç—å
available_feature_cols = [col for col in feature_columns if col in scaled_df.columns]
print(f"–î–æ—Å—Ç—É–ø–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è features: {available_feature_cols}")

if target_column not in scaled_df.columns:
    print(f"–ü–æ–º–∏–ª–∫–∞: –∫–æ–ª–æ–Ω–∫–∞ '{target_column}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞!")
    exit()

# –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
scaled_df[available_feature_cols] = feature_scaler.fit_transform(scaled_df[available_feature_cols])
scaled_df[[target_column]] = target_scaler.fit_transform(scaled_df[[target_column]])

print("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–∏—Ö:")
print(scaled_df.head())
print(f"Shape: {scaled_df.shape}")

def prepare_data(df, window_size=5):
    """
    –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –≤—ñ–∫–Ω–æ–º —á–∞—Å—É
    """
    X, y = [], []
    feature_cols = [col for col in df.columns if col != 'close']
    
    for i in range(len(df) - window_size):
        # –ë–µ—Ä–µ–º–æ features –∑–∞ window_size –¥–Ω—ñ–≤
        features = df[feature_cols].iloc[i:i+window_size].values
        # Target - —Ü—ñ–Ω–∞ –∑–∞–∫—Ä–∏—Ç—Ç—è –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ –¥–Ω—è
        target = df['close'].iloc[i+window_size]
        
        X.append(features.flatten())
        y.append(target)
    
    return np.array(X), np.array(y)

class MLPModel(nn.Module):
    def __init__(self, input_size, hidden_size1, hidden_size2=None, dropout_rate=0.2):
        super(MLPModel, self).__init__()
        
        layers = []
        layers.append(nn.Linear(input_size, hidden_size1))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout_rate))
        
        if hidden_size2:
            layers.append(nn.Linear(hidden_size1, hidden_size2))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            layers.append(nn.Linear(hidden_size2, 1))
        else:
            layers.append(nn.Linear(hidden_size1, 1))
            
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

def run_torch_model(model_name, model, X_train, y_train, X_test, y_test, 
                   target_scaler, results_dict, epochs=100, lr=0.001):
    """
    –ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∞ PyTorch –º–æ–¥–µ–ª—ñ
    """
    print(f"Training {model_name}...")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –≤ —Ç–µ–Ω–∑–æ—Ä–∏
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32).to(device)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
    
    model = model.to(device)
    criterion = torch.nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    # –ù–∞–≤—á–∞–Ω–Ω—è
    train_losses = []
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        output = model(X_train_tensor)
        loss = criterion(output, y_train_tensor)
        loss.backward()
        optimizer.step()
        
        train_losses.append(loss.item())
        
        if (epoch + 1) % 20 == 0:
            print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}")
    
    # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
    model.eval()
    with torch.no_grad():
        preds = model(X_test_tensor).cpu().numpy()
    
    # –ó–≤–æ—Ä–æ—Ç–Ω—î –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è (—Ç—ñ–ª—å–∫–∏ –¥–ª—è target)
    preds_inv = target_scaler.inverse_transform(preds)
    y_test_inv = target_scaler.inverse_transform(y_test.reshape(-1, 1))
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    rmse = np.sqrt(mean_squared_error(y_test_inv, preds_inv))
    mae = mean_absolute_error(y_test_inv, preds_inv)
    
    results_dict[model_name] = {
        'RMSE': rmse,
        'MAE': mae,
        'final_loss': train_losses[-1]
    }
    
    # –ì—Ä–∞—Ñ—ñ–∫ –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤
    plt.figure(figsize=(15, 6))
    
    # –ì—Ä–∞—Ñ—ñ–∫ 1: –ü—Ä–æ–≥–Ω–æ–∑–∏ vs –†–µ–∞–ª—å–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
    plt.subplot(1, 2, 1)
    plt.plot(y_test_inv.flatten()[:100], label='Actual', alpha=0.8)
    plt.plot(preds_inv.flatten()[:100], label='Predicted', alpha=0.8)
    plt.title(f"{model_name} - Predictions vs Actual")
    plt.xlabel("Time")
    plt.ylabel("Price")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ—ñ–∫ 2: Loss –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è
    plt.subplot(1, 2, 2)
    plt.plot(train_losses)
    plt.title(f"{model_name} - Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("MSE Loss")
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"RMSE: {rmse:.4f}, MAE: {mae:.4f}")
    print("-" * 50)

# –û—Å–Ω–æ–≤–Ω–∏–π –∫–æ–¥ –¥–ª—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
results = {}

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
window_sizes = [5, 10, 15]
hidden_sizes = [(64,), (128,), (64, 32), (128, 64), (256, 128)]
learning_rates = [0.001, 0.01]
epochs_list = [50, 100]

print("üöÄ –ü–æ—á–∞—Ç–æ–∫ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤ –∑ MLP –º–æ–¥–µ–ª—è–º–∏")
print("=" * 60)

# –û—Å–Ω–æ–≤–Ω–∏–π —Ü–∏–∫–ª –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
experiment_count = 0
total_experiments = len(window_sizes) * len(hidden_sizes) * len(learning_rates) * len(epochs_list)

for window_size in window_sizes:
    print(f"\nüìä Window size: {window_size}")
    
    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –ø–æ—Ç–æ—á–Ω–æ–≥–æ window_size
    X, y = prepare_data(scaled_df, window_size=window_size)
    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)
    
    print(f"Training set shape: {X_train.shape}")
    print(f"Test set shape: {X_test.shape}")
    
    input_size = X_train.shape[1]
    
    for hidden_config in hidden_sizes:
        for lr in learning_rates:
            for epochs in epochs_list:
                experiment_count += 1
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
                if len(hidden_config) == 1:
                    model = MLPModel(input_size=input_size, 
                                   hidden_size1=hidden_config[0])
                    model_name = f"MLP_w{window_size}_h{hidden_config[0]}_lr{lr}_e{epochs}"
                else:
                    model = MLPModel(input_size=input_size, 
                                   hidden_size1=hidden_config[0],
                                   hidden_size2=hidden_config[1])
                    model_name = f"MLP_w{window_size}_h{hidden_config[0]}-{hidden_config[1]}_lr{lr}_e{epochs}"
                
                print(f"\n[{experiment_count}/{total_experiments}] {model_name}")
                
                # –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
                run_torch_model(model_name, model, X_train, y_train, X_test, y_test, 
                              target_scaler, results, epochs=epochs, lr=lr)

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
print("\n" + "=" * 80)
print("üìà –ü–Ü–î–°–£–ú–û–ö –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í –í–°–Ü–• –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–Ü–í")
print("=" * 80)

# –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ RMSE
sorted_results = sorted(results.items(), key=lambda x: x[1]['RMSE'])

print(f"{'Model Name':<40} {'RMSE':<10} {'MAE':<10} {'Final Loss':<12}")
print("-" * 80)

for model_name, metrics in sorted_results:
    rmse = metrics['RMSE']
    mae = metrics['MAE']
    final_loss = metrics['final_loss']
    print(f"{model_name:<40} {rmse:<10.4f} {mae:<10.4f} {final_loss:<12.6f}")

# –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å
best_model = sorted_results[0]
print(f"\nüèÜ –ù–ê–ô–ö–†–ê–©–ê –ú–û–î–ï–õ–¨: {best_model[0]}")
print(f"   RMSE: {best_model[1]['RMSE']:.4f}")
print(f"   MAE: {best_model[1]['MAE']:.4f}")

# –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ window_size
print(f"\nüìä –ê–ù–ê–õ–Ü–ó –ü–û WINDOW SIZE:")
for ws in window_sizes:
    ws_results = [(name, metrics) for name, metrics in results.items() if f"_w{ws}_" in name]
    if ws_results:
        best_ws = min(ws_results, key=lambda x: x[1]['RMSE'])
        avg_rmse = np.mean([metrics['RMSE'] for _, metrics in ws_results])
        print(f"Window {ws}: –ù–∞–π–∫—Ä–∞—â–∏–π RMSE = {best_ws[1]['RMSE']:.4f}, –°–µ—Ä–µ–¥–Ω—ñ–π RMSE = {avg_rmse:.4f}")